# Introduction to Deep Learning

- Why now?

Picking out the building blocks, features from raw data, the algorithms have been here for decades.
But data available now are much more pervasive, and since they algorithms are compute hungry and extremely parallelizable.

1. Big Data 2. Harwdware 3. Software

- The Perceptron: Forward Propogation


$$
\hat{y} = g(w_0 \sum_{i=1}^{m} x_i w_i)
$$

$ \hat{y} $ is the output, g the non-linear activation function, x the input, g works on the linear combination of the inputs
$ w_0 $ is the bias term

$$
\hat{y} = g(w_0 + X^T W)
$$

Multi output Perceptron

$$
z_i = w_{0,i} + \sum_{j=1}^{m} x_j w_{j, i}
$$

```Python

class MyDenseLayer(tf.keras.layers.Layer):
    def __init(self, input_dim, output_dim):

        super(MyDenseLayer, self).__init__()

        # initialize weights and bias
        self.W = self.add_weight([input_dim, output_dim])
        self.b = self.add_weight([1, output_dim])

    def call(self, inputs):
        z = tf.matmul(inputs, self.W) + self.b
        output = tf.math.sigmoid(z)
        return output
```

'Deep' neural network -> we have multiple layers of these perceptrons

$$
z_{k,i} = w_{0, i}^{(k)} + \sum_{j=1}^{n_{k-1}}g(z_{k-1, j})w_{j, i}^{(k)}
$$

### Emperical loss: total loss over the entire dataset


$$
J(\mathbf{W}) = \frac{1}{n} \sum_{i=1}^{n} \mathcal{L} \Big( f(\mathbf{x}^{(i)}; \mathbf{W}), y^{(i)} \Big)
$$

- Binary cross entropy loss
cross entropy loss can be used with models that output probability between 0 and 1

$$
\mathbf{J}(\mathbf{W}) = - \frac{1}{n} \sum_{i=1}^{n} \Big[ y^{(i)} \log \hat{y}^{(i)} + (1 - y^{(i)}) \log (1 - \hat{y}^{(i)}) \Big]
$$

$$
y^{(i)} \doteq f(\mathbf{x}^{(i)}; \mathbf{W})
$$

- Mean Squared Error loss: for real valued outputs

$$
\mathbf{J}(\mathbf{W}) = \frac{1}{n} \sum_{i=1}^{n} \Big( y^{(i)} - \hat{y}^{(i)} \Big)^2
$$

### Loss Optimization

- find networks weights that acheive the lowest loss

$$
\mathbf{W}^* = \arg \min_{\mathbf{W}} J(\mathbf{W})
$$

### Gradient Descent

algorithm:

1. Initialize weights randomly ~ \( \mathcal{N}(0, \sigma^{2}) \)
2. Loop until convergence:
   3. Compute gradient, \( \frac{\partial J(\mathbf{W})}{\partial \mathbf{W}} \)
   4. Update weights,  
      \[
      \mathbf{W} \leftarrow \mathbf{W} - \eta \frac{\partial J(\mathbf{W})}{\partial \mathbf{W}}
      \]
   5. Return weights

learning rate: \( \eta \)

Computing gradients: Backpropagation

x -> z1 -> y_hat -> J(W)

The gradient of the loss function \( J(\mathbf{W}) \) with respect to a weight \( w_2 \) is given by:

\[
\frac{\partial J(\mathbf{W})}{\partial w_2} = \frac{\partial J(\mathbf{W})}{\partial \hat{y}} \times \frac{\partial \hat{y}}{\partial w_2}
\]

Each term represents a part of the backpropagation process:

- **\( \frac{\partial J(\mathbf{W})}{\partial \hat{y}} \)**  - The gradient of the loss function with respect to the predicted output.
- **\( \frac{\partial \hat{y}}{\partial w_2} \)** - The gradient of the predicted output with respect to the weight.

Applying the **chain rule** for multiple layers:

\[
\frac{\partial J(\mathbf{W})}{\partial w_1} = \frac{\partial J(\mathbf{W})}{\partial \hat{y}} \times \frac{\partial \hat{y}}{\partial w_1}
\]

By extending the chain rule further for deeper layers:

\[
\frac{\partial J(\mathbf{W})}{\partial w_1} = \frac{\partial J(\mathbf{W})}{\partial \hat{y}} \times \frac{\partial \hat{y}}{\partial z_1} \times \frac{\partial z_1}{\partial w_1}
\]

where:
- **\( \frac{\partial \hat{y}}{\partial z_1} \)**  - Gradient of the output with respect to the hidden neuron.
- **\( \frac{\partial z_1}{\partial w_1} \)**  - Gradient of the hidden neuron with respect to the weight.

- Loss functions can be difficult to optimize, if learning rate very small it can get stuck at the local minima, but too large might overshoot the local minima

- how to set the learning rate? try lots of different learning rates
- or designa n adaptive learning rate that "adapts" to the landscape

- Different ways of setting the learning rate:
    - SGD
    - Adam
    - Adadelta
    - adagrad
    - RMSprop

### **SGD**: Stochastic Gradient Descent

- instead of computing gradient over the entire dataset, pick a single point, compute the gradient and update the weights
- downside: noise, Stochastic
- middle ground: take a mini batch

so step 4:
$$
\frac{\partial J(\mathbf{W})}{\partial \mathbf{W}} = \frac{1}{B} \sum_{i=1}^{B} \frac{\partial J_{k}(\mathbf{W})}{\partial \mathbf{W}}
$$
- much faster than regular gradient descent
- smoother convergence, allows for larger learning rates
- mini batches also allow for parallelizing computation


### combat overfitting

Regularization: technique that contrains out optimization problem to discourage complex models


- Regularizaiton I: Dropout

    - during training, randomly set some activations to 0
    - forces networks to not rely on I node

- Regularization 2: Early stopping

    - stop training before we have a change to overfit, there will be a point where test loss plateus and starts increasing, training obviously keeps decreasing, stop training here
    - model agnostic