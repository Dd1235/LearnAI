{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOlrSnwyYIOe1+WGQiQ2AAq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dd1235/LearnAI/blob/main/mini_projects/GruSaganQuote/gru_generate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbDNpLLhdOxn",
        "outputId": "ededcc51-1655-4e0a-8873-d4218c6cbccd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LearnAI'...\n",
            "remote: Enumerating objects: 34309, done.\u001b[K\n",
            "remote: Counting objects: 100% (34309/34309), done.\u001b[K\n",
            "remote: Compressing objects: 100% (34273/34273), done.\u001b[K\n",
            "remote: Total 34309 (delta 53), reused 34275 (delta 25), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (34309/34309), 37.10 MiB | 9.96 MiB/s, done.\n",
            "Resolving deltas: 100% (53/53), done.\n",
            "Updating files: 100% (32499/32499), done.\n",
            "/content/LearnAI/mini_projects/GruSaganQuote\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Dd1235/LearnAI.git\n",
        "%cd LearnAI/mini_projects/GruSaganQuote"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUZbhL1Dd8Ls",
        "outputId": "401dbe1d-c608-4225-fa0e-d2c9c9b2c46b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30: 100% 1904/1904 [00:15<00:00, 120.84it/s]\n",
            "\n",
            "Epoch 1/30 | Loss: 1.0120\n",
            "Epoch 2/30: 100% 1904/1904 [00:15<00:00, 124.98it/s]\n",
            "\n",
            "Epoch 2/30 | Loss: 0.6734\n",
            "Epoch 3/30: 100% 1904/1904 [00:15<00:00, 121.91it/s]\n",
            "\n",
            "Epoch 3/30 | Loss: 0.6698\n",
            "Epoch 4/30: 100% 1904/1904 [00:15<00:00, 122.36it/s]\n",
            "\n",
            "Epoch 4/30 | Loss: 0.7868\n",
            "Epoch 5/30: 100% 1904/1904 [00:15<00:00, 119.98it/s]\n",
            "\n",
            "Epoch 5/30 | Loss: 1.0554\n",
            "Epoch 6/30: 100% 1904/1904 [00:16<00:00, 118.59it/s]\n",
            "\n",
            "Epoch 6/30 | Loss: 1.0971\n",
            "Epoch 7/30: 100% 1904/1904 [00:15<00:00, 119.65it/s]\n",
            "\n",
            "Epoch 7/30 | Loss: 1.0335\n",
            "Epoch 8/30: 100% 1904/1904 [00:15<00:00, 120.43it/s]\n",
            "\n",
            "Epoch 8/30 | Loss: 1.0532\n",
            "Epoch 9/30: 100% 1904/1904 [00:15<00:00, 121.15it/s]\n",
            "\n",
            "Epoch 9/30 | Loss: 1.0399\n",
            "Epoch 10/30: 100% 1904/1904 [00:15<00:00, 120.56it/s]\n",
            "\n",
            "Epoch 10/30 | Loss: 1.1302\n",
            "Epoch 11/30: 100% 1904/1904 [00:15<00:00, 119.96it/s]\n",
            "\n",
            "Epoch 11/30 | Loss: 1.3405\n",
            "Epoch 12/30: 100% 1904/1904 [00:15<00:00, 120.73it/s]\n",
            "\n",
            "Epoch 12/30 | Loss: 2.0510\n",
            "Epoch 13/30: 100% 1904/1904 [00:15<00:00, 121.32it/s]\n",
            "\n",
            "Epoch 13/30 | Loss: 2.0233\n",
            "Epoch 14/30: 100% 1904/1904 [00:15<00:00, 121.28it/s]\n",
            "\n",
            "Epoch 14/30 | Loss: 2.0267\n",
            "Epoch 15/30: 100% 1904/1904 [00:15<00:00, 121.35it/s]\n",
            "\n",
            "Epoch 15/30 | Loss: 2.0353\n",
            "Epoch 16/30: 100% 1904/1904 [00:15<00:00, 121.47it/s]\n",
            "\n",
            "Epoch 16/30 | Loss: 2.0562\n",
            "Epoch 17/30: 100% 1904/1904 [00:15<00:00, 121.52it/s]\n",
            "\n",
            "Epoch 17/30 | Loss: 2.0633\n",
            "Epoch 18/30: 100% 1904/1904 [00:15<00:00, 120.99it/s]\n",
            "\n",
            "Epoch 18/30 | Loss: 2.0435\n",
            "Epoch 19/30: 100% 1904/1904 [00:15<00:00, 121.66it/s]\n",
            "\n",
            "Epoch 19/30 | Loss: 2.0349\n",
            "Epoch 20/30: 100% 1904/1904 [00:15<00:00, 121.61it/s]\n",
            "\n",
            "Epoch 20/30 | Loss: 1.9954\n",
            "Epoch 21/30: 100% 1904/1904 [00:15<00:00, 121.22it/s]\n",
            "\n",
            "Epoch 21/30 | Loss: 1.9303\n",
            "Epoch 22/30: 100% 1904/1904 [00:15<00:00, 121.76it/s]\n",
            "\n",
            "Epoch 22/30 | Loss: 1.7942\n",
            "Epoch 23/30: 100% 1904/1904 [00:15<00:00, 121.86it/s]\n",
            "\n",
            "Epoch 23/30 | Loss: 1.6293\n",
            "Epoch 24/30: 100% 1904/1904 [00:15<00:00, 121.50it/s]\n",
            "\n",
            "Epoch 24/30 | Loss: 1.4542\n",
            "Epoch 25/30: 100% 1904/1904 [00:15<00:00, 121.63it/s]\n",
            "\n",
            "Epoch 25/30 | Loss: 1.3263\n",
            "Epoch 26/30: 100% 1904/1904 [00:15<00:00, 121.90it/s]\n",
            "\n",
            "Epoch 26/30 | Loss: 1.2052\n",
            "Epoch 27/30: 100% 1904/1904 [00:15<00:00, 121.66it/s]\n",
            "\n",
            "Epoch 27/30 | Loss: 1.1575\n",
            "Epoch 28/30: 100% 1904/1904 [00:15<00:00, 121.68it/s]\n",
            "\n",
            "Epoch 28/30 | Loss: 1.0835\n",
            "Epoch 29/30: 100% 1904/1904 [00:15<00:00, 121.95it/s]\n",
            "\n",
            "Epoch 29/30 | Loss: 1.0440\n",
            "Epoch 30/30: 100% 1904/1904 [00:15<00:00, 121.99it/s]\n",
            "\n",
            "Epoch 30/30 | Loss: 1.0784\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"sagan_gru.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "dNIqKk_heIqn",
        "outputId": "761a412e-9d63-47c7-8b89-a17738665f94"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_40b6279f-caba-4664-b155-1f93a5b33f63\", \"sagan_gru.pth\", 1677792)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from model.gru_model import CharGRU\n",
        "from utils import *"
      ],
      "metadata": {
        "id": "0a6-oGUEhgur"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(f\"Running on device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPkRLgYYiRHe",
        "outputId": "6f046d6c-db71-40db-c43c-281082e41d89"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = load_and_clean_text(\"carl_sagan_quotes.txt\")\n",
        "char2idx, idx2char = create_char_mappings(text)"
      ],
      "metadata": {
        "id": "p-rPJgx3iWzq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 256\n",
        "model = CharGRU(len(char2idx), hidden_size)\n",
        "model.load_state_dict(torch.load(\"sagan_gru.pth\", map_location=device))\n",
        "model.to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbFChX1niZSj",
        "outputId": "1b444bbe-17a6-410f-f908-ddc98cc6e88c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CharGRU(\n",
              "  (embed): Embedding(47, 256)\n",
              "  (gru): GRU(256, 256, batch_first=True)\n",
              "  (fc): Linear(in_features=256, out_features=47, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, start, char2idx, idx2char, length=300, temperature=1.0, device=\"cpu\"):\n",
        "    model.eval()\n",
        "    input_seq = torch.tensor([char2idx[c] for c in start.lower()], dtype=torch.long).unsqueeze(0).to(device)\n",
        "    hidden = model.init_hidden(1).to(device)\n",
        "    generated = start\n",
        "\n",
        "    for _ in range(length):\n",
        "        output, hidden = model(input_seq, hidden)\n",
        "        output_dist = torch.softmax(output[-1] / temperature, dim=0)\n",
        "        char_id = torch.multinomial(output_dist, 1).item()\n",
        "        next_char = idx2char[char_id]\n",
        "        generated += next_char\n",
        "        input_seq = torch.tensor([[char_id]], dtype=torch.long).to(device)\n",
        "\n",
        "    return generated"
      ],
      "metadata": {
        "id": "Uk6ZR2GtidLy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = \"we are\"\n",
        "output = generate(model, seed, char2idx, idx2char, length=300, temperature=0.8, device=device)\n",
        "print(f\"\\nGenerated Text:\\n{output}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4LowG8EigaM",
        "outputId": "235ef597-02f5-43cd-ffd6-5b60fbdeb1a6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated Text:\n",
            "we are on the entire umitical. that's insights in the plong to believe so a places if we moment to me, in the the futur did byou lon the enduracialting evidence. school decly usid religion. in reatend more gaspenderequire vistant indistinguish, there athat sound that resusem to the earth numos...howring t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate(model, \"on a moat\", char2idx, idx2char, length=300, temperature=0.8, device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "QD6h_Xwviimg",
        "outputId": "b3a40a50-709c-4bcb-acd2-a56a2ba57758"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"on a moater, well, so mindolate bill are bigor public qued hore a devers that many of the intelligence tend to support attempt tuals the fores. we've for sufficienting. our persones, the same sense for great arsuross the one has or ever god served have a planet for ones to find it hage quent their has ever m\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = generate(model, seed, char2idx, idx2char, length=300, temperature=0.4, device=device)"
      ],
      "metadata": {
        "id": "IyrEOT-BizGv"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output # lower temperature gives more coherant sentances"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "j9yhT_mmi8XB",
        "outputId": "9bd832f2-f63f-48d1-f101-fdd400f617c0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'we are the are the stars of science and the extray, but in many the fincledition is to be so than the imple the stars in a nation. in the end more sometimes reepletes and beginnal belief; if there on a place for the universe is made of the earth of mehow vast its and that it was wonder sometimes reason an'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate(model, seed, char2idx, idx2char, length=300, temperature=0.1, device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "De4WAXUQi893",
        "outputId": "d5b90adf-cafb-4f21-aa69-ab5f6e7cbbba"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'we are the could be so to thing our contrary to the inside of the stars. that are all of our progress have the cosmos are not in the universe is a predisch in a so in is a sunce of the see that the seems to me of the cosmos arguments and the same to be crazy, of new hypotheses have been universe in the un'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate(model, \"on a moat\", char2idx, idx2char, length=300, temperature=0.2, device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "9EMbIkqrjGtX",
        "outputId": "63348238-384e-4392-abd4-43aad7b1b725"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'on a moater the courage to the strul. for the idea of science is an are are are more construct ideas of the stars. there is a say for such a god it is a system technology that the standard its understo of our planet in a so get to be crite to set of the see that the stars. there is a religions in the in the '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7wojcEWdjM7A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}