# Attempt at Understanding Attention and Transformers/Notes

Plan:
[] understand properly including the math
[] Pytorch implementation
[] Write a "write up" explaining it in my own words


A transformer is a neural network
Go through a bunch of attention blocks and multi layer perceptron blobs.
Sample from the distribution.

"Embedding" a word : word -> vector.
E(woman) - E(man) = E(queen) - E(king) = E(neice) - E(nephew) = E(father) - E(mother)

E(hitler) + E(italy) - E(germany) = E(mussolini)
e(sushi) + e(germnay) - e(japan) = e(bratwurst)

so before attention -> Embedding
input = embedding + positional information


Embedding matrix: each column for each token
Total weights/parameters = embedding dimension * vocab size

They don't just represent individual words, that encodes information about the word. Like the word "King"'s embedding will be changed as you learn hes a King from the Chola dynasty vs when he is a King from a chess game.

We want ot "empower" a word to store context efficiently.

Context size: initially when you create an "array" out of your sentence, you just have the embedding of the word from the embedding matrix. Flows through networks, each vector evolves based on the context. You infer a lot about what a word means from surroundgin stuff, sometimes from a sentence many many sentences away.  "Context size" : limits how much text you can incorporate into making hte prediction.

Directed output: Probability distribution over the tokens that can occur next.

why use only the last embedding to make prediction? you use each one of those layers to make the next one.

Unembedding matrix: one tow for each word, as many elements as the embedding dimension. 

Softmax function: you want a sequence of numbers to act as a probability distribution, each value has to be between 0 and 1 and all have to add up to one.
But default outputs are not like this.
Softmax function: exp(x) / sum(exp(x))
Turn an arbitray vector into a probability distribution.

Temperature: add to the denominator of the exponents.
When T is larger you get more weight to the lower values.T = 0, all the weights goes to the maximum value.
T = 0, always goes to the max value, higher temperatoure, can choose more interesting, less prob ones.

Inputs: Logits
Outputs: Probabilities

You need to make sure the final word should have the full context. Imagine ..........., hence the murder was ___. You need the full context.

Single *head* of attention. Also encode the position of the word.
Series of computations -> more refined set of embeddings.

Query: another vector, but much smaller dimension

Wq * embedding = Q (transform the embedding to a lower dimension)

Query: embedding space -> query key space

Embeddings of ... "attend to" the embedding of ...

Embeddings of fluffly and blue attend to hte embedding of creature
Grid gives you a score , take a weighted sum in a column, compute a softmax along a column to normalize these values.

Attention pattern: Attention(Q,K,V) formula
divide by square root of dimension fo the key query space and apply softmax on each

Never allow later words influence the earlier ones.
Can't just set to zero so set all these to -inf before applying softmax - *masking*

Sparse attention mechanism
Multiply value matrix with embeddings to get valeu vectors, multply each value vector by the corresponding weights adn update the embedding.

# value params = # query params + # key params
esp in terms of multi head 

break into two matrices.
constraining hte value map to be a low rank transformation

Slef-attention head vs Cross attention head

cross attention: two types of data. Text in one language to one to another
Key and Query on different data
These remind me of on policy and off policy methods in RL

They crashed the car, there are a lot of implicaitons on the structure of the car.
Wizerd.. hogwards...hermoine, .... harry
vs 
sussex...queen...prince...harry

Since attention head, a full attention block has multi headed attention, each having their own Wk, Wq, Wv

The value map is factored into two matrices, value up and value down.
In papers, all the values up matrices from all the heads, output matrix.
Data flowing through a transformer goies through many copies of attention block and a multi layer perceptron.

d_query*d_embed*n_heads*n_layers

100s of billions of parameters
# 175 billion parameters

How do LLMs store facts


Vectors like in a high dimensinoal space.
embibe a much richer meaning

What is inside the MLP?

Each individual vectors -> series of operations. Linear ReLU, Linear, This all happens in parallel.

Multiply the vector by a very big matrix (Linear) and add a bias.
Pass the large intermediate vector to a non Linear function, eg ReLU
Mimics the behavious of the and gate.
Gelu.

Neurons of a transformer, combination of a linear step and a term wise non linear function.


Essentially, take a linear combination of the columns of the matrix and add the bias term. The columns are the weights of the neuron. 

Johnson-Lindestraus Lemma

independent ideas -> almost perpendicular, so model performance scales very well with size.

# Interlude?

simpsons paradox

Treatment A vs Treatment B
People who got treatment B survived more than people who got treatment A.
But patients with smaller kidney stones did better with treatment A.
Patients with larger kidney stones did better with treatment A.
How come?
not properly randomized. Moajority of patients with larger stones were given treatment A, and majority of patients with smaller stones were given treatment B.

cross entropy, one distribution to another.
it doesn't commute
measure of represetning how much two distributions differ.
-> KL divergence