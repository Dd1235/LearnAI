{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPDp/36HFKpqDtsSOC4DSDy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dd1235/LearnAI/blob/main/mini_projects/Siamese_Network_OmniglotSiameseNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0BXyWC5k78N",
        "outputId": "8fb895e0-21c7-4967-b782-319b0c1b2eaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LearnAI'...\n",
            "remote: Enumerating objects: 34242, done.\u001b[K\n",
            "remote: Counting objects: 100% (34242/34242), done.\u001b[K\n",
            "remote: Compressing objects: 100% (34224/34224), done.\u001b[K\n",
            "remote: Total 34242 (delta 24), reused 34225 (delta 10), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (34242/34242), 34.50 MiB | 14.40 MiB/s, done.\n",
            "Resolving deltas: 100% (24/24), done.\n",
            "Updating files: 100% (32489/32489), done.\n",
            "/content/LearnAI/mini_projects/Siamese_Network_Omniglot\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Dd1235/LearnAI.git\n",
        "%cd LearnAI/mini_projects/Siamese_Network_Omniglot"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n",
        "!ls \"Omniglot Dataset\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKTAbUSVlEF_",
        "outputId": "256c7b0a-1f0f-450c-8016-84e40723e9c3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Omniglot Dataset'   oneshot1.pdf   README.md\n",
            "images_background  images_evaluation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# it is bad practice to put entire 133MB of dataset in a folder like done here, will change later\n",
        "# this is simplified to make the training faster for demonstration purposes"
      ],
      "metadata": {
        "id": "ZZ2MtS9WlXF9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "\n",
        "data_path = \"Omniglot Dataset\"\n",
        "background_path = os.path.join(data_path, 'images_background')\n",
        "evaluation_path = os.path.join(data_path, 'images_evaluation')\n",
        "\n",
        "print(\"Background path:\", background_path)\n",
        "print(\"Evaluation path:\", evaluation_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kjdtnhi5xH2H",
        "outputId": "6f747315-884b-431d-e80b-e6c1401006ff"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Background path: Omniglot Dataset/images_background\n",
            "Evaluation path: Omniglot Dataset/images_evaluation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class OmniglotDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Loads pairs of images from the Omniglot background set for training.\n",
        "    Each item is (img1, img2, label) where label=1 if same class, else 0.\n",
        "    \"\"\"\n",
        "    def __init__(self, root, transform=None, num_pairs=30000):\n",
        "        \"\"\"\n",
        "        root: path to the background images folder\n",
        "        transform: optional transform (e.g. ToTensor, etc.)\n",
        "        num_pairs: how many pairs to generate for the dataset\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        self.all_images = []  # (image_path, character_id)\n",
        "        current_label = 0\n",
        "\n",
        "        alphabets = os.listdir(root)\n",
        "        for alpha in alphabets:\n",
        "            alpha_path = os.path.join(root, alpha)\n",
        "            if not os.path.isdir(alpha_path):\n",
        "                continue\n",
        "            chars = os.listdir(alpha_path)\n",
        "            for char in chars:\n",
        "                char_path = os.path.join(alpha_path, char)\n",
        "                if not os.path.isdir(char_path):\n",
        "                    continue\n",
        "                images_in_char = os.listdir(char_path)\n",
        "                for imgname in images_in_char:\n",
        "                    if imgname.endswith(\".png\"):\n",
        "                        full_path = os.path.join(char_path, imgname)\n",
        "                        self.all_images.append((full_path, current_label))\n",
        "                current_label += 1\n",
        "\n",
        "        self.num_chars = current_label  # total unique characters\n",
        "        # Group images by label for easy same-class sampling\n",
        "        from collections import defaultdict\n",
        "        self.images_by_label = defaultdict(list)\n",
        "        for img_path, lbl in self.all_images:\n",
        "            self.images_by_label[lbl].append(img_path)\n",
        "\n",
        "        self.all_labels = np.arange(self.num_chars)\n",
        "        self.num_pairs = num_pairs\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_pairs\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # half the time same class, half the time different\n",
        "        same_class = np.random.choice([0,1])\n",
        "\n",
        "        if same_class == 1:\n",
        "            # pick random label\n",
        "            label = np.random.choice(self.all_labels)\n",
        "            # pick two images from that label\n",
        "            imgs = random.sample(self.images_by_label[label], 2)\n",
        "            y = 1\n",
        "        else:\n",
        "            # pick two different labels\n",
        "            lbl1, lbl2 = np.random.choice(self.all_labels, 2, replace=False)\n",
        "            img1 = random.choice(self.images_by_label[lbl1])\n",
        "            img2 = random.choice(self.images_by_label[lbl2])\n",
        "            imgs = [img1, img2]\n",
        "            y = 0\n",
        "\n",
        "        img1 = Image.open(imgs[0]).convert('L')\n",
        "        img2 = Image.open(imgs[1]).convert('L')\n",
        "\n",
        "        if self.transform:\n",
        "            img1 = self.transform(img1)\n",
        "            img2 = self.transform(img2)\n",
        "\n",
        "        return (img1, img2, torch.tensor(y, dtype=torch.float32))\n"
      ],
      "metadata": {
        "id": "Vl8HK4_zxK4E"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images_by_character(root):\n",
        "    \"\"\"\n",
        "    Returns a dict: { (alphabet, character): [list of image paths] }\n",
        "    \"\"\"\n",
        "    from collections import defaultdict\n",
        "    data_dict = defaultdict(list)\n",
        "    alphabets = os.listdir(root)\n",
        "    for alpha in alphabets:\n",
        "        alpha_path = os.path.join(root, alpha)\n",
        "        if not os.path.isdir(alpha_path):\n",
        "            continue\n",
        "        chars = os.listdir(alpha_path)\n",
        "        for char in chars:\n",
        "            char_path = os.path.join(alpha_path, char)\n",
        "            if not os.path.isdir(char_path):\n",
        "                continue\n",
        "            images_in_char = os.listdir(char_path)\n",
        "            for imgname in images_in_char:\n",
        "                if imgname.endswith('.png'):\n",
        "                    full_path = os.path.join(char_path, imgname)\n",
        "                    data_dict[(alpha, char)].append(full_path)\n",
        "    return data_dict\n",
        "\n",
        "class OneShotEvaluation:\n",
        "    def __init__(self, eval_root, transform=None, n_way=20):\n",
        "        \"\"\"\n",
        "        Prepare data for n-way one-shot tasks from the evaluation set.\n",
        "        \"\"\"\n",
        "        self.eval_root = eval_root\n",
        "        self.transform = transform\n",
        "        self.n_way = n_way\n",
        "        self.data_dict = load_images_by_character(eval_root)\n",
        "        # We can convert the dict keys into a list for easier random sampling\n",
        "        self.all_keys = list(self.data_dict.keys())\n",
        "\n",
        "    def get_one_shot_batch(self, batch_size=1):\n",
        "        \"\"\"\n",
        "        Generate a batch of n-way one-shot tasks.\n",
        "        Returns a list of tuples: (test_image, support_images, correct_index)\n",
        "        \"\"\"\n",
        "        tasks = []\n",
        "        for _ in range(batch_size):\n",
        "            sampled_keys = random.sample(self.all_keys, self.n_way)\n",
        "            correct_class = 0\n",
        "            images_for_char0 = self.data_dict[sampled_keys[0]]\n",
        "            # pick test image:\n",
        "            test_img_path = random.choice(images_for_char0)\n",
        "\n",
        "            # pick a support image for each of the n_way classes\n",
        "            support_image_paths = []\n",
        "            for k in sampled_keys:\n",
        "                char_imgs = self.data_dict[k]\n",
        "                sup_img_path = random.choice(char_imgs)\n",
        "                support_image_paths.append(sup_img_path)\n",
        "\n",
        "            # load them into memory and transform\n",
        "            test_img = Image.open(test_img_path).convert('L')\n",
        "            if self.transform:\n",
        "                test_img = self.transform(test_img)\n",
        "\n",
        "            support_imgs = []\n",
        "            for sp in support_image_paths:\n",
        "                sp_img = Image.open(sp).convert('L')\n",
        "                if self.transform:\n",
        "                    sp_img = self.transform(sp_img)\n",
        "                support_imgs.append(sp_img)\n",
        "\n",
        "            tasks.append((test_img, support_imgs, correct_class))\n",
        "\n",
        "        return tasks"
      ],
      "metadata": {
        "id": "3T3QJeXl2aj0"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SiameseNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SiameseNetwork, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "\n",
        "        # Dummy input to compute flatten size\n",
        "        dummy = torch.zeros(1, 1, 105, 105)\n",
        "        dummy_out = self.features(dummy)\n",
        "        flatten_dim = dummy_out.view(1, -1).size(1)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(flatten_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128)\n",
        "        )\n",
        "\n",
        "    def forward_once(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        out1 = self.forward_once(x1)\n",
        "        out2 = self.forward_once(x2)\n",
        "        return out1, out2\n",
        "\n"
      ],
      "metadata": {
        "id": "IAMyVokPxZ6v"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using a contrasitive margin based loss like in a lot of tutorials in Siamese nn instead of Cross Entropy with L2 like in the paper\n",
        "\n",
        "So not implementing L2 with weight decay or momentum scheduling"
      ],
      "metadata": {
        "id": "J7hzLH1jykzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ContrastiveLoss(nn.Module):\n",
        "    def __init__(self, margin=2.0):\n",
        "        super(ContrastiveLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, out1, out2, label):\n",
        "        euclidean_distance = F.pairwise_distance(out1, out2)\n",
        "        loss_same = label * torch.pow(euclidean_distance, 2)\n",
        "        loss_diff = (1 - label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)\n",
        "        loss = torch.mean(loss_same + loss_diff) / 2\n",
        "        return loss"
      ],
      "metadata": {
        "id": "gLfoyl01yHij"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# paper describes more data augmentation, ie, Affine Distortion\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((105, 105)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_dataset = OmniglotDataset(\n",
        "    root=background_path,\n",
        "    transform=transform,\n",
        "    num_pairs=20000  # fewer pairs for demonstration\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "model = SiameseNetwork().to(device)\n",
        "criterion = ContrastiveLoss(margin=2.0)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4) # SGD used in paper\n",
        "\n",
        "num_epochs = 5\n",
        "# for demonstration, upto 200, and early stopped if no decrease in validation error after 20 epochs in paper\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, (img1, img2, label) in enumerate(train_loader):\n",
        "        img1, img2, label = img1.to(device), img2.to(device), label.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out1, out2 = model(img1, img2)\n",
        "        loss = criterion(out1, out2, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {running_loss/100:.4f}\")\n",
        "            running_loss = 0.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkgVOH9ays3m",
        "outputId": "21267bec-7fb1-4a69-a0d9-f0da26ff6c70"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Step [100/625], Loss: 0.3694\n",
            "Epoch [1/5], Step [200/625], Loss: 0.2994\n",
            "Epoch [1/5], Step [300/625], Loss: 0.2882\n",
            "Epoch [1/5], Step [400/625], Loss: 0.2838\n",
            "Epoch [1/5], Step [500/625], Loss: 0.2590\n",
            "Epoch [1/5], Step [600/625], Loss: 0.2454\n",
            "Epoch [2/5], Step [100/625], Loss: 0.2365\n",
            "Epoch [2/5], Step [200/625], Loss: 0.2297\n",
            "Epoch [2/5], Step [300/625], Loss: 0.2249\n",
            "Epoch [2/5], Step [400/625], Loss: 0.2210\n",
            "Epoch [2/5], Step [500/625], Loss: 0.2199\n",
            "Epoch [2/5], Step [600/625], Loss: 0.2113\n",
            "Epoch [3/5], Step [100/625], Loss: 0.2236\n",
            "Epoch [3/5], Step [200/625], Loss: 0.2180\n",
            "Epoch [3/5], Step [300/625], Loss: 0.2087\n",
            "Epoch [3/5], Step [400/625], Loss: 0.2001\n",
            "Epoch [3/5], Step [500/625], Loss: 0.1952\n",
            "Epoch [3/5], Step [600/625], Loss: 0.1982\n",
            "Epoch [4/5], Step [100/625], Loss: 0.1979\n",
            "Epoch [4/5], Step [200/625], Loss: 0.1996\n",
            "Epoch [4/5], Step [300/625], Loss: 0.1979\n",
            "Epoch [4/5], Step [400/625], Loss: 0.1967\n",
            "Epoch [4/5], Step [500/625], Loss: 0.1937\n",
            "Epoch [4/5], Step [600/625], Loss: 0.1879\n",
            "Epoch [5/5], Step [100/625], Loss: 0.1864\n",
            "Epoch [5/5], Step [200/625], Loss: 0.1836\n",
            "Epoch [5/5], Step [300/625], Loss: 0.1799\n",
            "Epoch [5/5], Step [400/625], Loss: 0.1906\n",
            "Epoch [5/5], Step [500/625], Loss: 0.1863\n",
            "Epoch [5/5], Step [600/625], Loss: 0.1780\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_oneshot(model, eval_loader, n_way=20, k_tasks=100):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    for _ in range(k_tasks):\n",
        "        tasks = eval_loader.get_one_shot_batch(batch_size=1)\n",
        "        test_img, support_imgs, correct_idx = tasks[0]\n",
        "\n",
        "        test_img = test_img.unsqueeze(0).to(device)\n",
        "        distances = []\n",
        "        with torch.no_grad():\n",
        "            feat_test = model.forward_once(test_img)\n",
        "            for j in range(n_way):\n",
        "                sup_img = support_imgs[j].unsqueeze(0).to(device)\n",
        "                feat_sup = model.forward_once(sup_img)\n",
        "                dist = F.pairwise_distance(feat_test, feat_sup)\n",
        "                distances.append(dist.item())\n",
        "\n",
        "        pred_idx = np.argmin(distances)\n",
        "        if pred_idx == correct_idx:\n",
        "            correct += 1\n",
        "\n",
        "    acc = correct / k_tasks * 100.0\n",
        "    return acc"
      ],
      "metadata": {
        "id": "qtR8bYkJ1P_I"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oneshot_eval_loader = OneShotEvaluation(evaluation_path, transform=transform, n_way=20)\n",
        "\n",
        "accuracy = evaluate_oneshot(model, oneshot_eval_loader, n_way=20, k_tasks=50)\n",
        "print(f\"One-shot 20-way accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dTb2luHzR6w",
        "outputId": "9c362482-c668-4739-8bc1-3b8ce1d31730"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-shot 20-way accuracy: 80.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.name \"Dd1235\"\n",
        "!git config --global user.email \"deepya1235@gmail.com\"\n"
      ],
      "metadata": {
        "id": "3C4YWQ9_1MI3"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !cd /content/LearnAI/mini_projects/Siamese_Network_Omniglot && git add SiameseNetwork.ipynb\n",
        "# !cd /content/LearnAI/mini_projects/Siamese_Network_Omniglot && git commit -m \"Add SiameseNetwork.ipynb notebook from Colab\"\n",
        "\n",
        "# from next time open using github\n",
        "# and use !git to add and commit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mWLhwoY7wcR",
        "outputId": "fd7ae04f-72e9-4879-8b35-67d2d6cac65b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: pathspec 'SiameseNetwork.ipynb' did not match any files\n",
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "nothing to commit, working tree clean\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmDToitI8HRN",
        "outputId": "bd778a0d-01bc-4978-c203-10ed4194ade4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LearnAI/mini_projects/Siamese_Network_Omniglot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gN9HT3Vh8ew-",
        "outputId": "83c92db2-fb72-4d82-f395-9c0803547401"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Omniglot Dataset'   oneshot1.pdf   README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EAEIBhOk8h8T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}